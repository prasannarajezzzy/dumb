{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: papers\\2103.00020.pdf\n",
      "Downloaded: papers\\2106.10336.pdf\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "def download_arxiv_paper(arxiv_id, download_dir=\"papers\"):\n",
    "    \"\"\"\n",
    "    Downloads a single paper from arXiv in PDF format.\n",
    "    \"\"\"\n",
    "    url = f\"https://arxiv.org/pdf/{arxiv_id}.pdf\"\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "    pdf_path = os.path.join(download_dir, f\"{arxiv_id}.pdf\")\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    with open(pdf_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    \n",
    "    return pdf_path\n",
    "\n",
    "# Example usage\n",
    "papers_to_download = [\"2103.00020\", \"2106.10336\"]  # Example arXiv IDs\n",
    "for pid in papers_to_download:\n",
    "    downloaded_pdf_path = download_arxiv_paper(pid)\n",
    "    print(f\"Downloaded: {downloaded_pdf_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def pdf_to_text(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, \"rb\") as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() or \"\"\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "pdf_texts = {}\n",
    "for pid in papers_to_download:\n",
    "    pdf_path = os.path.join(\"papers\", f\"{pid}.pdf\")\n",
    "    text = pdf_to_text(pdf_path)\n",
    "    pdf_texts[pid] = text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 95\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Basic cleaning: remove extra whitespace, newlines, etc.\n",
    "    # Customize cleaning steps as needed.\n",
    "    cleaned = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return cleaned\n",
    "\n",
    "def chunk_text(text, chunk_size=500, overlap=50):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        end = min(start + chunk_size, len(words))\n",
    "        chunk = \" \".join(words[start:end])\n",
    "        chunks.append(chunk)\n",
    "        start += (chunk_size - overlap)  # Overlap for context continuity\n",
    "    return chunks\n",
    "\n",
    "all_chunks = []\n",
    "for pid, text in pdf_texts.items():\n",
    "    c_text = clean_text(text)\n",
    "    chunks = chunk_text(c_text)\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        all_chunks.append({\n",
    "            \"paper_id\": pid,\n",
    "            \"chunk_index\": idx,\n",
    "            \"chunk_text\": chunk\n",
    "        })\n",
    "\n",
    "print(f\"Total chunks: {len(all_chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 3/3 [00:04<00:00,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index size: 95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Example embedding model\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Convert chunk texts to embeddings\n",
    "chunk_texts = [chunk[\"chunk_text\"] for chunk in all_chunks]\n",
    "embeddings = model.encode(chunk_texts, show_progress_bar=True)\n",
    "\n",
    "# Convert to float32 for FAISS\n",
    "embeddings = np.array(embeddings, dtype='float32')\n",
    "\n",
    "# Build a FAISS index\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)  # L2 distance index\n",
    "index.add(embeddings)\n",
    "print(f\"FAISS index size: {index.ntotal}\")\n",
    "\n",
    "# Keep track of metadata in the same order\n",
    "metadata_store = [ (all_chunks[i][\"paper_id\"], all_chunks[i][\"chunk_index\"]) \n",
    "                   for i in range(len(all_chunks)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 2103.00020 - Chunk 59:\n",
      "the ViT-L/14 model ﬁne-tuned on 336-by-336 pixel input images. EfﬁcietNet We use the nine models (B0-B8) from the original EfﬁcientNet paper (Tan & Le, 2019), as well as the noisy-student variants (B0...\n",
      "\n",
      "Paper 2103.00020 - Chunk 8:\n",
      "of the model. For the text encoder, we only scale the width of the model to be proportional to the calculated increase in width of the ResNet and do not scale the depth at all, as we found CLIP’s perf...\n",
      "\n",
      "Paper 2103.00020 - Chunk 2:\n",
      "and pre-training approaches, VirTex (Desai & Johnson, 2020), ICMLM (Bulent Sariyildiz et al., 2020), and Con- VIRT (Zhang et al., 2020) have recently demonstrated the potential of transformer-based la...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_top_k_chunks(query, k=3):\n",
    "    query_embedding = model.encode([query], show_progress_bar=False)\n",
    "    query_embedding = np.array(query_embedding, dtype='float32')\n",
    "\n",
    "    # Search FAISS index\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    \n",
    "    retrieved_chunks = []\n",
    "    for dist, idx in zip(distances[0], indices[0]):\n",
    "        paper_id, chunk_index = metadata_store[idx]\n",
    "        chunk_text = all_chunks[idx][\"chunk_text\"]\n",
    "        retrieved_chunks.append({\n",
    "            \"paper_id\": paper_id,\n",
    "            \"chunk_index\": chunk_index,\n",
    "            \"chunk_text\": chunk_text,\n",
    "            \"distance\": dist\n",
    "        })\n",
    "    return retrieved_chunks\n",
    "\n",
    "# Example usage\n",
    "user_query = \"What are the latest advancements in Transformer models?\"\n",
    "top_chunks = get_top_k_chunks(user_query, k=3)\n",
    "for chunk in top_chunks:\n",
    "    print(f\"Paper {chunk['paper_id']} - Chunk {chunk['chunk_index']}:\\n{chunk['chunk_text'][:200]}...\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, pipeline\n",
    "from ollama import chat\n",
    "def load_llama_model(model_name=\"meta-llama/Llama-2-7b-chat-hf\"):\n",
    "    # For large models, you might need device_map=\"auto\" and torch_dtype=torch.float16\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",  # automatically spread across GPUs\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    generate_pipeline = pipeline(\n",
    "        \"text-generation\", \n",
    "        model=model, \n",
    "        tokenizer=tokenizer, \n",
    "        max_length=2048, \n",
    "        temperature=0.7, \n",
    "        do_sample=True,  # or False, depending on your preference\n",
    "        top_k=50,\n",
    "        repetition_penalty=1.2\n",
    "    )\n",
    "    return generate_pipeline\n",
    "\n",
    "def generate_answer_with_context_llama(query, top_chunks, generate_pipeline):\n",
    "    # Construct a prompt that includes the retrieved context\n",
    "    context_text = \"\\n\\n\".join(\n",
    "        f\"[Source: paper={c['paper_id']}, chunk={c['chunk_index']}] {c['chunk_text']}\"\n",
    "        for c in top_chunks\n",
    "    )\n",
    "\n",
    "    # You can structure your prompt however you like:\n",
    "    system_prompt = (\n",
    "        \"You are a helpful AI assistant. Use only the text from the sources below to answer:\\n\"\n",
    "        f\"{context_text}\\n\\n\"\n",
    "        \"Now answer the user question. \"\n",
    "        \"If the answer cannot be found, say you are not sure. \"\n",
    "        \"Include references to [Source: paper=xxx, chunk=yyy] wherever relevant.\\n\"\n",
    "        f\"User's question: {query}\\n\\n\"\n",
    "    )\n",
    "\n",
    "    # Call the pipeline\n",
    "    # output = generate_pipeline(system_prompt, max_new_tokens=512)[0][\"generated_text\"]\n",
    "    response: ChatResponse = chat(model='llama3.2', max_tokens=512, messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': system_prompt,\n",
    "    },\n",
    "    ])\n",
    "    generated_text = response['message']['content']\n",
    "    print(\"Generated Text:\", generated_text)\n",
    "    # The pipeline will return the entire prompt plus generation. \n",
    "    # You might want to strip out the prompt or parse the relevant part.\n",
    "    # generated_answer = output[len(system_prompt):].strip()\n",
    "    \n",
    "\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__func__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__self__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "generate_answer_with_context_llama() missing 1 required positional argument: 'generate_pipeline'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_answer_with_context_llama\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_chunks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(answer)\n",
      "\u001b[1;31mTypeError\u001b[0m: generate_answer_with_context_llama() missing 1 required positional argument: 'generate_pipeline'"
     ]
    }
   ],
   "source": [
    "answer = generate_answer_with_context_llama(user_query, top_chunks)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ollama import  chat\n",
    "from ollama import ChatResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
