import logging
from pathlib import Path

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

from dotenv import load_dotenv

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

env_path = Path('.') / '.env'
load_dotenv(dotenv_path=env_path)

MODEL_NAME = "meta-llama/LLaMA"

try:
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)
    model.eval()
    logger.info("LLaMA model loaded successfully.")
except Exception as e:
    logger.error(f"Error loading LLaMA model: {e}")
    exit(1)

def generate_response(prompt: str, max_new_tokens: int = 150) -> str:
    """
    Generate a response from the LLaMA model based on the given prompt.

    Args:
        prompt (str): The input text prompt.
        max_new_tokens (int): The maximum number of tokens to generate.

    Returns:
        str: The generated response.
    """
    try:
        inputs = tokenizer(prompt, return_tensors="pt")
        with torch.no_grad():
            outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        logger.info(f"Generated response: {response}")
        return response
    except Exception as e:
        logger.error(f"Error generating response with LLaMA: {e}")
        return "Sorry, I encountered an error while generating a response."

def main():
    print("LLaMA Model Tester")
    print("Type 'exit' to quit.\n")
    while True:
        prompt = input("Enter your prompt: ")
        if prompt.lower() in ['exit', 'quit']:
            print("Exiting the LLaMA Model Tester.")
            break
        response = generate_response(prompt)
        print(f"Response: {response}\n")

if __name__ == "__main__":
    main()
